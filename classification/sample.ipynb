{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import timm\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':512,\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    'BATCH_SIZE':32,\n",
    "    'SEED':3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"Final Project\", \n",
    "    entity=\"aitech4_cv3\",\n",
    "    name='classification_resnet50',\n",
    "    config = {\n",
    "        \"lr\" : CFG['LEARNING_RATE'],\n",
    "        \"epoch\" : CFG['EPOCHS'],\n",
    "        \"batch_size\" : CFG['BATCH_SIZE'],\n",
    "    }\n",
    "    )\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = pd.read_csv('/opt/ml/data_csv/data.csv')\n",
    "label = data_file['info']\n",
    "data =  data_file.drop(['info',\"Unnamed: 0\"], axis=1)\n",
    "X_train, y_train, X_test, y_test = train_test_split(data, label, test_size=0.2, random_state=CFG['SEED'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([A.RandomResizedCrop(CFG['IMG_SIZE'],CFG['IMG_SIZE'],(0.5,0.5),(1,1)),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "\n",
    "val_transform = A.Compose([\n",
    "                            A.RandomResizedCrop(CFG['IMG_SIZE'],CFG['IMG_SIZE'],(0.5,0.5),(1,1)),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "y_train = y_train.reset_index()\n",
    "y_test = y_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_paths, labels, transforms=None):\n",
    "        self.img_paths = img_paths['filepath']\n",
    "        self.labels = labels['info']\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_paths[index]\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)['image']\n",
    "            \n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[index]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(X_train, X_test, train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(y_train, y_test, val_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = timm.create_model(model_name='resnet34', pretrained=True)\n",
    "        self.fc = nn.Linear(1000,num_classes)\n",
    "        #self.classifier1 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.fc(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = torch.round(y_pred)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    model_preds = []\n",
    "    true_labels = []\n",
    "    \n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, label in iter(test_loader):\n",
    "            img, label = img.float().to(device), label.float().to(device).reshape(-1,1)\n",
    "            \n",
    "            model_pred = model(img)\n",
    "\n",
    "            loss = criterion(model_pred, label)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            val_acc.append(calculate_accuracy(model_pred, label).item())\n",
    "            \n",
    "            true_labels += label.detach().cpu().numpy().tolist()\n",
    "\n",
    "    return np.mean(val_loss), np.mean(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_of_epoch(start, end):\n",
    "    time_per_epoch = end - start\n",
    "    time_per_epoch_min = int(time_per_epoch / 60)\n",
    "    time_per_epoch_sec = int(time_per_epoch -(time_per_epoch_min*60))\n",
    "    return time_per_epoch_min, time_per_epoch_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, test_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, CFG[\"EPOCHS\"]+1):\n",
    "        model.train()\n",
    "        start_time = time.monotonic()\n",
    "        train_loss = []\n",
    "        epoch_acc = 0\n",
    "        for step,(img, label) in enumerate(train_loader): #tqdm(iter(train_loader)\n",
    "\n",
    "            img, label = img.float().to(device), label.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            label = label.view(-1,1)\n",
    "\n",
    "            model_pred = model(img)\n",
    "\n",
    "            acc = calculate_accuracy(model_pred, label)\n",
    "\n",
    "            loss = criterion(model_pred, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            if (step + 1) % 5 == 0:\n",
    "                print(f'Epoch [{epoch}], Step [{step+1}], Train Loss : [{round(loss.item(),4):.5f}] Train acc : [{acc:.5f}]')\n",
    "\n",
    "        end_time = time.monotonic()\n",
    "        epoch_min, epoch_sec = time_of_epoch(start_time, end_time)\n",
    "        train_loss_m = np.mean(train_loss)\n",
    "        epoch_acc += acc.item()\n",
    "        val_loss, val_acc = validation(model, criterion, test_loader, device)\n",
    "\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{train_loss_m:.5f}] Val Loss : [{val_loss:.5f}] Val acc : [{val_acc:.5f}],Time : {epoch_min}m {epoch_sec}s')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        if best_score < val_acc:\n",
    "            best_model = model\n",
    "            best_score = val_acc\n",
    "            print(f\"save_best_pth EPOCH {epoch}\")\n",
    "            torch.save(model.state_dict(),\"../../model_save_dir/best.pth\")\n",
    "        \n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Step [5], Train Loss : [0.56700] Train acc : [0.71875]\n",
      "Epoch [1], Step [10], Train Loss : [0.38140] Train acc : [0.81250]\n",
      "Epoch [1], Step [15], Train Loss : [0.42520] Train acc : [0.84375]\n",
      "Epoch [1], Step [20], Train Loss : [0.46230] Train acc : [0.71875]\n",
      "Epoch [1], Step [25], Train Loss : [0.34590] Train acc : [0.84375]\n",
      "Epoch [1], Train Loss : [0.50589] Val Loss : [1.10370] Val acc : [0.72356],Time : 0m 33s\n",
      "save_best_pth EPOCH 1\n",
      "Epoch [2], Step [5], Train Loss : [0.15340] Train acc : [0.93750]\n",
      "Epoch [2], Step [10], Train Loss : [0.36660] Train acc : [0.84375]\n",
      "Epoch [2], Step [15], Train Loss : [0.33620] Train acc : [0.87500]\n",
      "Epoch [2], Step [20], Train Loss : [0.30120] Train acc : [0.81250]\n",
      "Epoch [2], Step [25], Train Loss : [0.25290] Train acc : [0.90625]\n",
      "Epoch [2], Train Loss : [0.33035] Val Loss : [0.53539] Val acc : [0.78846],Time : 0m 32s\n",
      "save_best_pth EPOCH 2\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel()\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = None\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_paths = \"테스트 이미지 경로\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, img_paths, labels, transforms=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_paths\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)['image']\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomTestDataset(test_img_paths, None, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, img_path, device):\n",
    "    model = BaseModel()\n",
    "    model.load_state_dict(torch.load(\"/opt/ml/model_save_dir/best.pt\", map_location=device))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model_preds = []\n",
    "    img_paths = []\n",
    "    img_paths.append(img_path)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for img in iter(test_loader):\n",
    "            img = img.float().to(device)\n",
    "            model_pred = model(img)\n",
    "            model_preds.append(torch.round(model_pred).detach().cpu().numpy())\n",
    "    \n",
    "    print('Done.')\n",
    "    return model_preds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "preds = inference(infer_model, test_img_paths, device)\n",
    "print(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
